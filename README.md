# Practical Deep Learning 

Learning Projects to Get Started with Deep Learning

[![GitHub license](https://img.shields.io/github/license/dcarpintero/deep-learning-notebooks.svg)](https://github.com/dcarpintero/deep-learning-notebooks/blob/master/LICENSE)
[![GitHub contributors](https://img.shields.io/github/contributors/dcarpintero/deep-learning-notebooks.svg)](https://GitHub.com/dcarpintero/deep-learning-notebooks/graphs/contributors/)
[![GitHub issues](https://img.shields.io/github/issues/dcarpintero/deep-learning-notebooks.svg)](https://GitHub.com/dcarpintero/deep-learning-notebooks/issues/)
[![GitHub pull-requests](https://img.shields.io/github/issues-pr/dcarpintero/deep-learning-notebooks.svg)](https://GitHub.com/dcarpintero/deep-learning-notebooks/pulls/)
[![PRs Welcome](https://img.shields.io/badge/PRs-welcome-brightgreen.svg?style=flat-square)](http://makeapullrequest.com)

[![GitHub watchers](https://img.shields.io/github/watchers/dcarpintero/deep-learning-notebooks.svg?style=social&label=Watch)](https://GitHub.com/dcarpintero/deep-learning-notebooks/watchers/)
[![GitHub forks](https://img.shields.io/github/forks/dcarpintero/deep-learning-notebooks.svg?style=social&label=Fork)](https://GitHub.com/dcarpintero/deep-learning-notebooks/network/)
[![GitHub stars](https://img.shields.io/github/stars/dcarpintero/deep-learning-notebooks.svg?style=social&label=Star)](https://GitHub.com/dcarpintero/deep-learning-notebooks/stargazers/)

## 01. Annotated Multi-Layer Perceptron

Build, Train, and Deploy a Multi-Layer Perceptron to classify handwritten Digits. It  implements from scratch the following modules: **linear layer**, **relu-activation-function**, **sequential-layer**, **flattening-layer**, **basic optimizer**, and **learner**. The model achieves 95.6% accuracy with `15 training epochs` and `batch size = 64`.

Experiment with different layer configurations, batch sizes, and training epochs. Understand the role of gradients, backpropagation, activation functions, flattening, and the importance of breaking simmetry with random parameter initialization.

[Annotated Notebook](./multi_layer_perceptron.ipynb) |
[Open in Colab](https://colab.research.google.com/github/dcarpintero/deep-learning-notebooks/blob/main/multi_layer_perceptron.ipynb) |
[Try in HuggingFace Spaces](https://huggingface.co/spaces/dcarpintero/mlp-digit-classifier) 

`[deep-learning]` `[perceptron]` `[backpropagation]` `[gradient-descend]` `[linear-layer]` `[relu]` `[optimizer]` `[mnist]` `[multi-class]` `[universal-approximation-theorem]`

<p align="center">
  <img src="./static/mnist.hg.png">
</p>